'use strict'

// This loads the env variables to a process.env object
require('dotenv').load()

// require fs module
const fs = require('fs')

// require mime module
const mime = require('mime')

// require path module
const path = require('path')

// require crypto module
const crypto = require('crypto')

// require mongoose
const mongoose = require('./../app/middleware/mongoose')

// require upload model
const Upload = require('./../app/models/upload')

// require aws-sdk module
const AWS = require('aws-sdk')

// create a new instance of AWS.S3 object
const s3 = new AWS.S3()

// testing that dotenv module works and loads bucket name
// script should accept file as 2nd argument
// print to make sure it does

const file = {
  path: process.argv[2],
  name: process.argv[3],
  bucket: process.env.AWS_S3_BUCKET_NAME
}
console.log('this is process.env.AWS_S3_BUCKET_NAME', process.env.AWS_S3_BUCKET_NAME)
const randomBytesPromise = function () {
  return new Promise((resolve, reject) => {
    // Generates cryptographically strong pseudo-random data.
   // The size argument is a number indicating the number of bytes to generate.
   // If a callback function is provided, the bytes are generated asynchronously
   // and the callback function is invoked with two arguments: err and buf
   // If an error occurs, err will be an Error object;
   // otherwise it is null.
   // The buf argument is a Buffer containing the generated bytes.
   // https://nodejs.org/api/crypto.html#crypto_crypto_randombytes_size_callback
    crypto.randomBytes(16, function (error, buffer) {
      if (error) {
        reject(error)
      } else {
        resolve(buffer.toString('hex'))
      }
    })
  })
}

const s3Upload = function (options) {
  // use node fs module to create a read stream
  // for our image file
  // https://www.sitepoint.com/basics-node-js-streams/
  const stream = fs.createReadStream(options.path)

  // use node mime module to get image mime type
  // https://www.npmjs.com/package/mime
  const contentType = mime.lookup(options.path)

  const ext = path.extname(options.path)

  const folder = new Date().toISOString().split('T')[0]

  const params = {
    ACL: 'public-read',
    // Bucket: options.bucket,
    Bucket: process.env.AWS_S3_BUCKET_NAME,
    Body: stream,
    Key: `${folder}/${options.name}${ext}` || 'default_name',
    ContentType: contentType
  }
  console.log('This is options.bucket ', options.bucket)
  console.log('This is process.env.AWS_S3_BUCKET_NAME', process.env.AWS_S3_BUCKET_NAME)

  // return a promise object that is resolved or rejected,
  // based on the response from s3.upload
  return new Promise((resolve, reject) => {
    // attempt s3.upload knowing it will fail to ensure the
    // module is required correctly
    s3.upload(params, function (error, data) {
      if (error) {
        reject(error)
      } else {
        resolve(data)
      }
    })
  })
}

const awsUpload = function (options) {
// generate random Bytes to use for name and start promise chain
  return randomBytesPromise()
  .then((buffer) => {
    // set file name to buffer returned from randomBytesPromise
    options.name = buffer

    // return file so it is passed as argument to s3Upload
    return options
  })
  .then(s3Upload)
  .catch(console.error)
}

module.exports = awsUpload
